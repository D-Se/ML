[{"path":"index.html","id":"ml","chapter":"1 ML ","heading":"1 ML ","text":"   Sincerity Aspiration, Perseverance IntegrityThis repository contains analysis, utilities workflows NJU 020205D17 Machine Learning course.currently  recruiting! Interested join? Send email Donald Seinen : 2869830202@qq.com, add QQ / WeChat. questions project, Github Discussions available.see books (ML) source code, see Github repo.","code":""},{"path":"index.html","id":"project-goals","chapter":"1 ML ","heading":"1.1 Project Goals","text":"Deliver scalable, reproducible continuously integrated data analysis compendium :\nDeploys Machine Learning algorithms,\nVisualizes model performances & statistical inference,\nCommunicates findings target audiences multiple formats.\nDeliver scalable, reproducible continuously integrated data analysis compendium :Deploys Machine Learning algorithms,Deploys Machine Learning algorithms,Visualizes model performances & statistical inference,Visualizes model performances & statistical inference,Communicates findings target audiences multiple formats.Communicates findings target audiences multiple formats.Display best practices edge machine learning knowledge using R, placing importance :\nPackaging approaches using version control,\nDeploying computationally efficient algorithms,\nInterfacing modelling & reporting API’s,\nAdopting functional programming mindset.\nDisplay best practices edge machine learning knowledge using R, placing importance :Packaging approaches using version control,Packaging approaches using version control,Deploying computationally efficient algorithms,Deploying computationally efficient algorithms,Interfacing modelling & reporting API’s,Interfacing modelling & reporting API’s,Adopting functional programming mindset.Adopting functional programming mindset.","code":""},{"path":"index.html","id":"project-deliverables","chapter":"1 ML ","heading":"1.2 Project deliverables","text":"data analysis compendium,data analysis compendium,R package containing target-based tidymodels reporting tools,R package containing target-based tidymodels reporting tools,pipeline-generated bookdown book & shiny app,pipeline-generated bookdown book & shiny app,Persistent Github repository future publishing.Persistent Github repository future publishing.\nFigure 1.1: Project Outline\n","code":""},{"path":"index.html","id":"project-visualization","chapter":"1 ML ","heading":"1.3 Project visualization","text":"click drag image inspect move nodes!image already 25 000 different models, implemented using 12 modelling packages! node represents step targets core pipeline.example, set models fails, red, code needs looked . Another example dependency book, parameterized.keras neural nets, MARS, Random Forests, SVM already part ML API. project uses R & C++ computation, JavaScript, HTML & CSS reporting, YAML, TeX AWK text formatting. Whether like modelling, visualizing, communicating, projects offers chance explore best R practices. best part? don’t leave RStudio .NJU community. open beginners, advanced users alike. Extensive help documentation written accomodate group member comfortable productive possible!Curious know , want see live coding example? Send email add QQ WeChat!","code":""},{"path":"overview.html","id":"overview","chapter":"2 Overview","heading":"2 Overview","text":"","code":""},{"path":"overview.html","id":"quick-start","chapter":"2 Overview","heading":"2.1 Quick start","text":"ML scalable data analysis compendium interfaces well known APIs like tidyverse, tidymodels, shiny, bookdown.ML already quite rich functionality.\n: 1. Uses functional programming - fetches, validates, transforms cleans data,Specifies modelling workflows,Specifies modelling workflows,Implements workflows asynchronously,Implements workflows asynchronously,Collect metrics, visualizes model performance,Collect metrics, visualizes model performance,Constructs book, Shiny API!Constructs book, Shiny API!finally, exports finalized models end users convenient objects inspection sharing.finally, exports finalized models end users convenient objects inspection sharing.start modelling, useWant retrieve intermediate step analysis pipeline? Sure! example xgboost tree model, displaying hyperparameters need tuning.team finds best models present engaging way: book!","code":"\ndevtools::install_github(\"D-Se/ML\")\nML::analysis$models$xgb_spec\n#> Boosted Tree Model Specification (regression)\n#> \n#> Main Arguments:\n#>   trees = tune()\n#>   min_n = tune()\n#>   tree_depth = tune()\n#>   learn_rate = tune()\n#>   loss_reduction = tune()\n#>   sample_size = tune()\n#> \n#> Computational engine: xgboost"},{"path":"overview.html","id":"github-repo","chapter":"2 Overview","heading":"2.2 Github repo","text":"Github repo project lives. , can find, among many others, following files.project split segments, ending finishing deliverable. steps can found milestones section repo. milestones made collections issues. tasks assigned project members, can added person access rights repo. labels placed issues make easy find assign tasks. issue allocated difficulty priority label, labels appropriate.","code":"├── .github  >>> github workflows, automated checking\n├── R/  >>> directory for ML package functions.\n├──── ... >>> regular R package functions. o sub-directory allowed.\n├── inst/  >>> supplementary materials for ML package\n├── man/  >>> automatically generated package documentation [NO EDIT]\n├──── figures/ >>> images used\n├── src/  >>> compiled (C++) code used in ML package\n├── tests/  >>> unit tests for package functions\n├──── testthat/ >>> directory of unit test\n├── vignettes/  >>> long-form documentation\n├── .Rbuildignore/ >>> package utility: instruction set to ignore non-standard files\n├── .gitignore  >>> instruction set to RStudio to ignore files in directory for version control\n├── DESCRIPTION  >>> R package metadata file\n├── ML.Rproj  >>> RStudio file [NO EDIT]\n├── NAMESPACE  >>> automatically generated R package build file [NO EDIT]\n├── README.Rmd  >>> Github introduction file with executable R code\n├── README.md >>> automatically generated Github introduction file [NO EDIT]\n├── run.sh  >>> shell script to execture pipeline\n├── run.R  >>>\n├── _targets.R  # core pipeline\n└── report.Rmd  "},{"path":"overview.html","id":"project-planning","chapter":"2 Overview","heading":"2.3 Project Planning","text":"large scale approach multiple people contribute models, code, opinions et cetera important keep track , things going.keep focus straight, automation tools available, Github commit-tracking give insights exactly , .Hmm, see might issue! Let’s deal Github!\nFigure 2.1: Github repository issues pane\n","code":"#!/usr/bin/bash sh\n# count lines of code per contributor to github repo in main .rmd files\ncontr () {\n  local perfile=\"false\";\n  if [[ $1 = \"-f\" ]]; then\n  perfile=\"true\";\n  shift;\n  fi;\n  if [[ $# -eq 0 ]]; then\n          echo \"no files given!\" 1>&2;\n        return 1;\n        else\n          local f; {\n            for f in \"$@\";\n            do\n            echo \"$f\";\n            git blame --show-email \"$f\" |\n              sed -nE 's/^[^ ]* *.<([^>]*)>.*$/: \\1/p' |\n              sort | uniq -c | sort -r -nk1;\n            done\n          } | if [[ \"$perfile\" = \"true\" ]]; then\n        tee /tmp/s.txt;\n        else\n          tee /tmp/s.txt > /dev/null;\n        fi;\n        awk -v FS='*: *' '/^ *[0-9]/{sums[$2] += $1} END {\n        for (i in sums) printf(\"%7s : %s\\n\", sums[i], i)\n        }' /tmp/s.txt |\n          sort -r -nk1;\n        fi\n}\nfind . -iname '*.Rmd' | while read file; do contr -f \"$file\"; done > dev/counts.txt"},{"path":"model-performance.html","id":"model-performance","chapter":"3 Model performance","heading":"3 Model performance","text":"example constructed sample data, training 12 different ML models.quick overview, obtained node pipeline.check predictions close actual observations, inspect another node pipeline.Looks good. error metric associated ?benefit making package.\n- Simply run ML::results get best performing model straight R session\n- Look documentation running ?results.","code":"\nlibrary(ggplot2)\ntargets::tar_read(ranking) +\n  theme_bw()\ntargets::tar_read(visual_verify) +\n  theme_bw()\ntargets::tar_read(metrics)\n#> # A tibble: 2 x 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard       3.72  Preprocessor1_Model1\n#> 2 rsq     standard       0.949 Preprocessor1_Model1\nML::results\n#> $predictions\n#> # A tibble: 249 x 3\n#>    sample_row model_prediction actual_observation\n#>         <int>            <dbl>              <dbl>\n#>  1          3            18.9               17.3 \n#>  2          5             3.81               2.33\n#>  3          7            21.2               20.6 \n#>  4         14             9.21              10.4 \n#>  5         18            34.0               33.3 \n#>  6         22            35.9               36.6 \n#>  7         23            32.0               31.0 \n#>  8         24            31.2               28.9 \n#>  9         29            19.9               18.2 \n#> 10         30            37.1               29.1 \n#> # ... with 239 more rows\n#> \n#> $workflow\n#> == Workflow [trained] ==========================================================\n#> Preprocessor: Variables\n#> Model: boost_tree()\n#> \n#> -- Preprocessor ----------------------------------------------------------------\n#> Outcomes: compressive_strength\n#> Predictors: everything()\n#> \n#> -- Model -----------------------------------------------------------------------\n#> ##### xgb.Booster\n#> Handle is invalid! Suggest using xgb.Booster.complete\n#> raw: 3.4 Mb \n#> call:\n#>   xgboost::xgb.train(params = list(eta = 0.0441995347243183, max_depth = 13L, \n#>     gamma = 7.77839129518202e-09, colsample_bytree = 1, colsample_bynode = 1, \n#>     min_child_weight = 6L, subsample = 0.155629670358263, objective = \"reg:squarederror\"), \n#>     data = x$data, nrounds = 1862L, watchlist = x$watchlist, \n#>     verbose = 0, nthread = 1)\n#> params (as set within xgb.train):\n#>   eta = \"0.0441995347243183\", max_depth = \"13\", gamma = \"7.77839129518202e-09\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"6\", subsample = \"0.155629670358263\", objective = \"reg:squarederror\", nthread = \"1\", validate_parameters = \"TRUE\"\n#> callbacks:\n#>   cb.evaluation.log()\n#> # of features: 8 \n#> niter: 1862\n#> nfeatures : 8 \n#> evaluation_log:\n#>     iter training_rmse\n#>        1     36.682041\n#>        2     35.311272\n#> ---                   \n#>     1861      1.508187\n#>     1862      1.508148"},{"path":"appendix-a.html","id":"appendix-a","chapter":"A Appendix A","heading":"A Appendix A","text":"something","code":""},{"path":"author.html","id":"author","chapter":"About the Authors","heading":"About the Authors","text":"","code":""},{"path":"author.html","id":"donald-seinen","chapter":"About the Authors","heading":"Donald Seinen","text":"","code":""},{"path":"report_a11c3970.html","id":"report_a11c3970","chapter":"B report_a11c3970","heading":"B report_a11c3970","text":"page depends parameters, content dynamic, can \nmanipulated shiny. built template \nparameters get inserted, built.","code":"## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -38.550 -13.998  -4.306  10.530 104.458 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -41.2159    19.3090  -2.135   0.0344 *  \n## Wind         -2.5986     0.5543  -4.688 6.14e-06 ***\n## Temp          1.4024     0.2063   6.798 2.35e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.4 on 150 degrees of freedom\n## Multiple R-squared:  0.451,  Adjusted R-squared:  0.4437 \n## F-statistic: 61.62 on 2 and 150 DF,  p-value: < 2.2e-16"}]
